{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas\n",
    "import nltk\n",
    "import numpy\n",
    "from blist import sorteddict, sortedset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-dd3a5725a1c1>:4: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pandas.set_option('display.max_colwidth', -1)\n"
     ]
    }
   ],
   "source": [
    "pandas.set_option('display.max_rows', None)\n",
    "pandas.set_option('display.max_columns', None)\n",
    "pandas.set_option('display.width', None)\n",
    "pandas.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\\\]^_`{|}~\\n\"\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "path = os.getcwd()+'/TelevisionNews'\n",
    "\n",
    "def wildcard(word, bigrams, st):\n",
    "    word = \"$\"+word+\"$\"\n",
    "    for i in range(len(word)-1):\n",
    "        bi = word[i:i+2]\n",
    "        if bi not in bigrams.keys():\n",
    "            bigrams[bi] = sortedset()\n",
    "            bigrams[bi].add(st)\n",
    "        elif st not in bigrams[bi]:\n",
    "            bigrams[bi].add(st)\n",
    "\n",
    "def process(row, bigrams):\n",
    "    data = numpy.char.lower(row)\n",
    "    for i in range(len(symbols)):\n",
    "        data = numpy.char.replace(data, symbols[i], '')\n",
    "    data = numpy.char.replace(data, ',', '')\n",
    "    data = numpy.char.replace(data, \"'\", \"\")\n",
    "    words = nltk.tokenize.word_tokenize(str(data))\n",
    "    new_data = \"\"\n",
    "    for w in words:\n",
    "        st = stemmer.stem(w)\n",
    "        wildcard(w, bigrams, st)\n",
    "        new_data = new_data + \" \" + st\n",
    "    data = numpy.char.strip(new_data)\n",
    "    return data\n",
    "\n",
    "def createFullIndex(column):\n",
    "    index = sorteddict()\n",
    "    documents = sorteddict()\n",
    "    docindex = sorteddict()\n",
    "    bigrams = sorteddict()\n",
    "    doc = 0\n",
    "    files_all = sorted(os.listdir(path))\n",
    "    for filename in files_all:\n",
    "        print(\"Document:\", doc, filename)\n",
    "        documents[doc] = filename\n",
    "        df = pandas.read_csv(path+'/'+filename)\n",
    "        r = 0\n",
    "        temp = sorteddict()\n",
    "        for row in df[column]:\n",
    "            pos = 0\n",
    "            data = nltk.word_tokenize(str(process(row, bigrams)))\n",
    "            for tok in data:\n",
    "                if tok in index:\n",
    "                #creating fullindex\n",
    "                    if doc in index[tok]:\n",
    "                        index[tok][doc].add(r)\n",
    "                    else:\n",
    "                        index[tok][doc] = sortedset()\n",
    "                        index[tok][doc].add(r)\n",
    "                else:\n",
    "                #creating fullindex\n",
    "                    index[tok] = sorteddict()\n",
    "                    index[tok][doc] = sortedset()\n",
    "                    index[tok][doc].add(r)\n",
    "                if tok in temp:\n",
    "                    #creating docindex\n",
    "                    if r in temp[tok].keys():\n",
    "                        temp[tok][r].append(pos)\n",
    "                    else:\n",
    "                        temp[tok][r] = [pos]\n",
    "                else:\n",
    "                    #creating docindex\n",
    "                    temp[tok] = sorteddict()\n",
    "                    temp[tok][r] = [pos]\n",
    "                pos = pos+1\n",
    "            r = r+1 \n",
    "        docindex[doc] = temp\n",
    "        doc = doc+1\n",
    "    return index, documents, docindex, bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document: 0 BBCNEWS.201701.csv\n",
      "Document: 1 BBCNEWS.201702.csv\n",
      "Document: 2 BBCNEWS.201703.csv\n",
      "Document: 3 BBCNEWS.201704.csv\n",
      "Document: 4 BBCNEWS.201705.csv\n",
      "Document: 5 BBCNEWS.201706.csv\n",
      "Document: 6 BBCNEWS.201707.csv\n",
      "Document: 7 BBCNEWS.201708.csv\n",
      "Document: 8 BBCNEWS.201709.csv\n",
      "Document: 9 BBCNEWS.201710.csv\n",
      "Document: 10 BBCNEWS.201711.csv\n",
      "Document: 11 BBCNEWS.201712.csv\n",
      "Document: 12 BBCNEWS.201801.csv\n",
      "Document: 13 BBCNEWS.201802.csv\n",
      "Document: 14 BBCNEWS.201803.csv\n",
      "Document: 15 BBCNEWS.201804.csv\n",
      "Document: 16 BBCNEWS.201805.csv\n",
      "Document: 17 BBCNEWS.201806.csv\n",
      "Document: 18 BBCNEWS.201807.csv\n",
      "Document: 19 BBCNEWS.201808.csv\n",
      "Document: 20 BBCNEWS.201809.csv\n",
      "Document: 21 BBCNEWS.201810.csv\n",
      "Document: 22 BBCNEWS.201811.csv\n",
      "Document: 23 BBCNEWS.201812.csv\n",
      "Document: 24 BBCNEWS.201901.csv\n",
      "Document: 25 BBCNEWS.201902.csv\n",
      "Document: 26 BBCNEWS.201903.csv\n",
      "Document: 27 BBCNEWS.201904.csv\n",
      "Document: 28 BBCNEWS.201905.csv\n",
      "Document: 29 BBCNEWS.201906.csv\n",
      "Document: 30 BBCNEWS.201907.csv\n",
      "Document: 31 BBCNEWS.201908.csv\n",
      "Document: 32 BBCNEWS.201909.csv\n",
      "Document: 33 BBCNEWS.201910.csv\n",
      "Document: 34 BBCNEWS.201911.csv\n",
      "Document: 35 BBCNEWS.201912.csv\n",
      "Document: 36 BBCNEWS.202001.csv\n",
      "Document: 37 CNN.200907.csv\n",
      "Document: 38 CNN.200908.csv\n",
      "Document: 39 CNN.200909.csv\n",
      "Document: 40 CNN.200911.csv\n",
      "Document: 41 CNN.200912.csv\n",
      "Document: 42 CNN.201001.csv\n",
      "Document: 43 CNN.201002.csv\n",
      "Document: 44 CNN.201003.csv\n",
      "Document: 45 CNN.201004.csv\n",
      "Document: 46 CNN.201005.csv\n",
      "Document: 47 CNN.201006.csv\n",
      "Document: 48 CNN.201007.csv\n",
      "Document: 49 CNN.201008.csv\n",
      "Document: 50 CNN.201009.csv\n",
      "Document: 51 CNN.201010.csv\n",
      "Document: 52 CNN.201011.csv\n",
      "Document: 53 CNN.201012.csv\n",
      "Document: 54 CNN.201101.csv\n",
      "Document: 55 CNN.201102.csv\n",
      "Document: 56 CNN.201103.csv\n",
      "Document: 57 CNN.201104.csv\n",
      "Document: 58 CNN.201105.csv\n",
      "Document: 59 CNN.201106.csv\n",
      "Document: 60 CNN.201107.csv\n",
      "Document: 61 CNN.201108.csv\n",
      "Document: 62 CNN.201109.csv\n",
      "Document: 63 CNN.201110.csv\n",
      "Document: 64 CNN.201111.csv\n",
      "Document: 65 CNN.201112.csv\n",
      "Document: 66 CNN.201201.csv\n",
      "Document: 67 CNN.201202.csv\n",
      "Document: 68 CNN.201203.csv\n",
      "Document: 69 CNN.201204.csv\n",
      "Document: 70 CNN.201205.csv\n",
      "Document: 71 CNN.201206.csv\n",
      "Document: 72 CNN.201207.csv\n",
      "Document: 73 CNN.201208.csv\n",
      "Document: 74 CNN.201209.csv\n",
      "Document: 75 CNN.201210.csv\n",
      "Document: 76 CNN.201211.csv\n",
      "Document: 77 CNN.201212.csv\n",
      "Document: 78 CNN.201301.csv\n",
      "Document: 79 CNN.201302.csv\n",
      "Document: 80 CNN.201303.csv\n",
      "Document: 81 CNN.201304.csv\n",
      "Document: 82 CNN.201305.csv\n",
      "Document: 83 CNN.201306.csv\n",
      "Document: 84 CNN.201307.csv\n",
      "Document: 85 CNN.201308.csv\n",
      "Document: 86 CNN.201309.csv\n",
      "Document: 87 CNN.201310.csv\n",
      "Document: 88 CNN.201311.csv\n",
      "Document: 89 CNN.201312.csv\n",
      "Document: 90 CNN.201401.csv\n",
      "Document: 91 CNN.201402.csv\n",
      "Document: 92 CNN.201403.csv\n",
      "Document: 93 CNN.201404.csv\n",
      "Document: 94 CNN.201405.csv\n",
      "Document: 95 CNN.201406.csv\n",
      "Document: 96 CNN.201407.csv\n",
      "Document: 97 CNN.201408.csv\n",
      "Document: 98 CNN.201409.csv\n",
      "Document: 99 CNN.201410.csv\n",
      "Document: 100 CNN.201411.csv\n",
      "Document: 101 CNN.201412.csv\n",
      "Document: 102 CNN.201501.csv\n",
      "Document: 103 CNN.201502.csv\n",
      "Document: 104 CNN.201503.csv\n",
      "Document: 105 CNN.201504.csv\n",
      "Document: 106 CNN.201505.csv\n",
      "Document: 107 CNN.201506.csv\n",
      "Document: 108 CNN.201507.csv\n",
      "Document: 109 CNN.201508.csv\n",
      "Document: 110 CNN.201509.csv\n",
      "Document: 111 CNN.201510.csv\n",
      "Document: 112 CNN.201511.csv\n",
      "Document: 113 CNN.201512.csv\n",
      "Document: 114 CNN.201601.csv\n",
      "Document: 115 CNN.201602.csv\n",
      "Document: 116 CNN.201603.csv\n",
      "Document: 117 CNN.201604.csv\n",
      "Document: 118 CNN.201605.csv\n",
      "Document: 119 CNN.201606.csv\n",
      "Document: 120 CNN.201607.csv\n",
      "Document: 121 CNN.201608.csv\n",
      "Document: 122 CNN.201609.csv\n",
      "Document: 123 CNN.201610.csv\n",
      "Document: 124 CNN.201611.csv\n",
      "Document: 125 CNN.201612.csv\n",
      "Document: 126 CNN.201701.csv\n",
      "Document: 127 CNN.201702.csv\n",
      "Document: 128 CNN.201703.csv\n",
      "Document: 129 CNN.201704.csv\n",
      "Document: 130 CNN.201705.csv\n",
      "Document: 131 CNN.201706.csv\n",
      "Document: 132 CNN.201707.csv\n",
      "Document: 133 CNN.201708.csv\n",
      "Document: 134 CNN.201709.csv\n",
      "Document: 135 CNN.201710.csv\n",
      "Document: 136 CNN.201711.csv\n",
      "Document: 137 CNN.201712.csv\n",
      "Document: 138 CNN.201801.csv\n",
      "Document: 139 CNN.201802.csv\n",
      "Document: 140 CNN.201803.csv\n",
      "Document: 141 CNN.201804.csv\n",
      "Document: 142 CNN.201805.csv\n",
      "Document: 143 CNN.201806.csv\n",
      "Document: 144 CNN.201807.csv\n",
      "Document: 145 CNN.201808.csv\n",
      "Document: 146 CNN.201809.csv\n",
      "Document: 147 CNN.201810.csv\n",
      "Document: 148 CNN.201811.csv\n",
      "Document: 149 CNN.201812.csv\n",
      "Document: 150 CNN.201901.csv\n",
      "Document: 151 CNN.201902.csv\n",
      "Document: 152 CNN.201903.csv\n",
      "Document: 153 CNN.201904.csv\n",
      "Document: 154 CNN.201905.csv\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-e83a1fce79d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbigrams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreateFullIndex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Snippet\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-49-82d863dac4e6>\u001b[0m in \u001b[0;36mcreateFullIndex\u001b[1;34m(column)\u001b[0m\n\u001b[0;32m     65\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m                     \u001b[1;31m#creating docindex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m                     \u001b[0mtemp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtok\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorteddict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m                     \u001b[0mtemp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtok\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m                 \u001b[0mpos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\blist\\_sorteddict.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, *args, **kw)\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msorteddict\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sortedkeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_key\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sortedkeys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msortedset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\blist\\_sortedlist.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, iterable, key)\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_blist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mblist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_blist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_blist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mblist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "index, documents, docindex, bigrams = createFullIndex(\"Snippet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json.dumps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wqprocess(row):\n",
    "    data = numpy.char.lower(row)\n",
    "    for i in range(len(symbols)):\n",
    "        data = numpy.char.replace(data, symbols[i], '')\n",
    "    data = numpy.char.replace(data, ',', '')\n",
    "    data = numpy.char.replace(data, \"'\", \"\")\n",
    "    words = nltk.tokenize.word_tokenize(str(data))\n",
    "    new_data = \"\"\n",
    "    for w in words:\n",
    "        st = stemmer.stem(w)\n",
    "        wildcard(w, bigrams, st)\n",
    "        new_data = new_data + \" \" + st\n",
    "    data = numpy.char.strip(new_data)\n",
    "    return data\n",
    "\n",
    "def query(query):\n",
    "    if '*' in query:\n",
    "        parts = query.split('*')\n",
    "        if '' in parts:\n",
    "            if parts[0] == '':\n",
    "                parts = [parts[1]+\"$\"]\n",
    "            elif parts[1] == '':\n",
    "                parts = [\"$\"+parts[0]]\n",
    "        else:\n",
    "            parts[0] = \"$\"+parts[0]\n",
    "            parts[1] = parts[1]+\"$\"\n",
    "        op = 1\n",
    "    elif len(query.split())==1:\n",
    "        parts = [query]\n",
    "        op = 2\n",
    "    else:\n",
    "        parts = query.split()\n",
    "        op = 3\n",
    "    return op, parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wildcard(words):\n",
    "    ws = sortedset()\n",
    "    for w in words:\n",
    "        for i in range(len(w)-1):\n",
    "            bi = w[i:i+2]\n",
    "            if bi not in bigrams.keys():\n",
    "                return \"not there\"\n",
    "            elif len(ws)==0:\n",
    "                ws = bigrams[bi]\n",
    "                continue\n",
    "            else:\n",
    "                ws = set.intersection(set(bigrams[bi]), ws)\n",
    "    final = sortedset()\n",
    "    for i in words:\n",
    "        e, s = i.split('$')\n",
    "        for i in ws:\n",
    "            if i.startswith(s) and i.endswith(e):\n",
    "                final.add(i)\n",
    "    print(final)\n",
    "    #TO DO: word query for all words in final\n",
    "    for i in final:\n",
    "        if i in index.keys():\n",
    "            temp = index[i]\n",
    "            for doc in temp.keys():\n",
    "                df = pandas.read_csv(path+\"/\"+documents[doc])\n",
    "                for row in temp[doc]:\n",
    "                    print(documents[doc],\" | \", list(df.iloc[[row]]['Snippet'])[0],\" | \",list(df.iloc[[row]]['URL'])[0],\" | \", list(df.iloc[[row]]['IAShowID'])[0],\" | \",'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordQuery(query):\n",
    "    cleaned = str(process(query))\n",
    "    if cleaned in index.keys():\n",
    "        temp = index[cleaned]\n",
    "        for doc in temp.keys():\n",
    "            df = pandas.read_csv(path+\"/\"+documents[doc])\n",
    "            for row in temp[doc]:\n",
    "                print(documents[doc],\" | \", list(df.iloc[[row]]['Snippet'])[0],\" | \",list(df.iloc[[row]]['URL'])[0],\" | \", list(df.iloc[[row]]['IAShowID'])[0],\" | \",'\\n')\n",
    "#wordQuery(\"legislature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#phrase query\n",
    "\n",
    "def getDocs(term):\n",
    "    if term in index.keys():\n",
    "        return index[term]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def getCommon(dind1, dind2):\n",
    "    ans = []\n",
    "    for i in dind1:\n",
    "        if i in dind2:\n",
    "            ans.append(i)\n",
    "    return ans\n",
    "\n",
    "def getCommonRows(rows1, rows2, k):\n",
    "    rows = []\n",
    "    for i in rows1.keys():\n",
    "        if i in rows2.keys():\n",
    "            pos1 = rows1[i]\n",
    "            pos2 = rows2[i]\n",
    "            j1 = 0\n",
    "            j2 = 0\n",
    "            while j1<len(pos1) and j2<len(pos2):\n",
    "                if pos2[j2] - pos1[j1] == k:\n",
    "                    rows.append(i)\n",
    "                    j1 = j1+1\n",
    "                    j2 = j2+1\n",
    "                elif pos1[j1]<pos2[j2]:\n",
    "                    j1 = j1+1\n",
    "                else:\n",
    "                    j2 = j2+1\n",
    "    return rows\n",
    "\n",
    "def getCommonData(docs, term1, term2, k):\n",
    "    ans = {}\n",
    "    for i in docs:\n",
    "        docin = docindex[i]\n",
    "        rows1 = docin[term1]\n",
    "        rows2 = docin[term2]\n",
    "        #print(i)\n",
    "        if len(rows1.keys())<len(rows2.keys()):\n",
    "            rows = getCommonRows(rows1, rows2, k)\n",
    "        else:\n",
    "            rows = getCommonRows(rows2, rows1, k)\n",
    "        if len(rows):\n",
    "            ans[i] = rows\n",
    "    return ans\n",
    "\n",
    "#def createDict(results, data):\n",
    "#    for i in data.keys():\n",
    "#        for j in data[i]:\n",
    "#            temp = str(i)+\"$\"+str(j)\n",
    "#            if temp in results:\n",
    "#                results[temp] += 1\n",
    "#            else:\n",
    "#                results[temp] = 1\n",
    "\n",
    "def createDict(results, data):\n",
    "    for i in data.keys():\n",
    "        if i not in results.keys():\n",
    "            results[i] = []\n",
    "        for j in data[i]:\n",
    "            results[i].append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    occs = sorteddict()\\n    for i in results.keys():\\n        if results[i] in occs:\\n            occs[results[i]].append(i)\\n        else:\\n            occs[results[i]] = [i]\\n\\n    print(occs)\\n    cnt = 10000\\n    t = 0\\n    for i in reversed(occs.keys()):\\n        print(i)\\n        for j in occs[i]:\\n            if cnt == 0:\\n                break\\n            doc, row = j.split(\\'$\\')\\n            df = pandas.read_csv(path+\\'/\\'+documents[int(doc)])\\n            cur = df.iloc[[int(row)]]\\n            print(doc,\" | \", row, \" | \",list(cur[\\'Snippet\\'])[0],\" | \", list(cur[\\'URL\\'])[0], \" | \")\\n            cnt = cnt - 1 \\n            t = t+1\\n        if cnt == 0:\\n            break\\n    print(t)'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#MUST BREAK INTO FUNCTIONS AND A MAIN FUNCTION\n",
    "\n",
    "pquery = \"major report released\"\n",
    "\n",
    "def pqprocess(row):\n",
    "    data = numpy.char.lower(row)\n",
    "    for i in range(len(symbols)):\n",
    "        data = numpy.char.replace(data, symbols[i], '')\n",
    "    data = numpy.char.replace(data, ',', '')\n",
    "    data = numpy.char.replace(data, \"'\", \"\")\n",
    "    words = nltk.tokenize.word_tokenize(str(data))\n",
    "    new_data = \"\"\n",
    "    for w in words:\n",
    "        st = stemmer.stem(w)\n",
    "        new_data = new_data + \" \" + st\n",
    "    data = numpy.char.strip(new_data)\n",
    "    return data\n",
    "\n",
    "def phraseQuery(pquery):\n",
    "    pcleaned = str(pqprocess(pquery)).split()\n",
    "    results = sorteddict()\n",
    "    for i in range(len(pcleaned)):\n",
    "        j = i+1\n",
    "        dind1 = getDocs(pcleaned[i])\n",
    "        if len(dind1)==0:\n",
    "            continue\n",
    "        while j<len(pcleaned):\n",
    "            dind2 = getDocs(pcleaned[j]) \n",
    "            if len(dind2)==0:\n",
    "                j = j+1\n",
    "                continue\n",
    "            if len(dind1)<=len(dind2):\n",
    "                docs = getCommon(dind1, dind2)\n",
    "                #print(len(docs))\n",
    "                data = getCommonData(docs, pcleaned[i], pcleaned[j], j-i)\n",
    "\n",
    "            else:\n",
    "                docs = getCommon(dind2, dind1)\n",
    "                data = getCommonData(docs, pcleaned[j], pcleaned[i], i-j)\n",
    "            print(data)\n",
    "            createDict(results, data)\n",
    "            j = j+1\n",
    "    return results\n",
    "\n",
    "'''    occs = sorteddict()\n",
    "    for i in results.keys():\n",
    "        if results[i] in occs:\n",
    "            occs[results[i]].append(i)\n",
    "        else:\n",
    "            occs[results[i]] = [i]\n",
    "\n",
    "    print(occs)\n",
    "    cnt = 10000\n",
    "    t = 0\n",
    "    for i in reversed(occs.keys()):\n",
    "        print(i)\n",
    "        for j in occs[i]:\n",
    "            if cnt == 0:\n",
    "                break\n",
    "            doc, row = j.split('$')\n",
    "            df = pandas.read_csv(path+'/'+documents[int(doc)])\n",
    "            cur = df.iloc[[int(row)]]\n",
    "            print(doc,\" | \", row, \" | \",list(cur['Snippet'])[0],\" | \", list(cur['URL'])[0], \" | \")\n",
    "            cnt = cnt - 1 \n",
    "            t = t+1\n",
    "        if cnt == 0:\n",
    "            break\n",
    "    print(t)'''\n",
    "#phraseQuery(pquery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#latest phrase query\n",
    "\n",
    "def getDocs(term):\n",
    "    if term in index.keys():\n",
    "        return index[term]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def getCommon(dind1, dind2):\n",
    "    ans = []\n",
    "    for i in dind1:\n",
    "        if i in dind2:\n",
    "            ans.append(i)\n",
    "    return ans\n",
    "\n",
    "def getCommonRows(rows1, rows2, k):\n",
    "    rows = []\n",
    "    for i in rows1.keys():\n",
    "        if i in rows2.keys():\n",
    "            pos1 = rows1[i]\n",
    "            pos2 = rows2[i]\n",
    "            j1 = 0\n",
    "            j2 = 0\n",
    "            while j1<len(pos1) and j2<len(pos2):\n",
    "                if pos2[j2] - pos1[j1] == k:\n",
    "                    rows.append(i)\n",
    "                    j1 = j1+1\n",
    "                    j2 = j2+1\n",
    "                elif pos1[j1]<pos2[j2]:\n",
    "                    j1 = j1+1\n",
    "                else:\n",
    "                    j2 = j2+1\n",
    "    return rows\n",
    "\n",
    "def getCommonData(docs, term1, term2, k):\n",
    "    ans = {}\n",
    "    for i in docs:\n",
    "        docin = docindex[i]\n",
    "        rows1 = docin[term1]\n",
    "        rows2 = docin[term2]\n",
    "        #print(i)\n",
    "        if len(rows1.keys())<len(rows2.keys()):\n",
    "            rows = getCommonRows(rows1, rows2, k)\n",
    "        else:\n",
    "            rows = getCommonRows(rows2, rows1, k)\n",
    "        if len(rows):\n",
    "            ans[i] = rows\n",
    "    return ans\n",
    "\n",
    "def createDict(results, data):\n",
    "    for i in data.keys():\n",
    "        if i not in results.keys():\n",
    "            results[i] = []\n",
    "        for j in data[i]:\n",
    "            results[i].append(j)\n",
    "            \n",
    "def pqprocess(row):\n",
    "    data = numpy.char.lower(row)\n",
    "    for i in range(len(symbols)):\n",
    "        data = numpy.char.replace(data, symbols[i], '')\n",
    "    data = numpy.char.replace(data, ',', '')\n",
    "    data = numpy.char.replace(data, \"'\", \"\")\n",
    "    words = nltk.tokenize.word_tokenize(str(data))\n",
    "    new_data = \"\"\n",
    "    for w in words:\n",
    "        st = stemmer.stem(w)\n",
    "        new_data = new_data + \" \" + st\n",
    "    data = numpy.char.strip(new_data)\n",
    "    return data\n",
    "\n",
    "def phraseQuery(pquery):\n",
    "    pcleaned = str(pqprocess(pquery)).split()\n",
    "    results = sorteddict()\n",
    "    for i in range(len(pcleaned)):\n",
    "        j = i+1\n",
    "        dind1 = getDocs(pcleaned[i])\n",
    "        if len(dind1)==0:\n",
    "            continue\n",
    "        while j<len(pcleaned):\n",
    "            dind2 = getDocs(pcleaned[j]) \n",
    "            if len(dind2)==0:\n",
    "                j = j+1\n",
    "                continue\n",
    "            if len(dind1)<=len(dind2):\n",
    "                docs = getCommon(dind1, dind2)\n",
    "                data = getCommonData(docs, pcleaned[i], pcleaned[j], j-i)\n",
    "\n",
    "            else:\n",
    "                docs = getCommon(dind2, dind1)\n",
    "                data = getCommonData(docs, pcleaned[j], pcleaned[i], i-j)\n",
    "            createDict(results, data)\n",
    "            j = j+1\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sorteddict({9: [14, 24, 44, 56, 57, 89, 91, 102, 138, 147, 151, 165, 212, 371, 372, 389, 390], 18: [210], 20: [10, 99, 106, 114, 117, 178, 322, 370], 21: [267], 22: [159, 394, 416, 550, 560, 577, 580, 1004], 26: [280, 299, 542, 217, 518, 994, 1239], 30: [23, 636, 732, 827, 985, 1231, 1413], 31: [156, 161, 381, 949], 40: [207], 47: [21], 85: [36, 37, 36, 37], 91: [11], 92: [10], 93: [20, 142], 100: [12], 128: [161], 146: [66], 147: [17, 396, 17, 215, 301, 396, 252, 334, 392], 148: [56], 220: [18, 111, 134, 205], 274: [35, 37, 141, 145, 174], 284: [697], 345: [142, 204], 346: [14, 292, 308], 347: [158, 165, 685, 694, 694, 117, 165, 340, 407, 531], 349: [84], 386: [129, 130], 389: [52], 401: [24, 438, 24, 438, 147, 380], 410: [161, 736], 413: [127]})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phraseQuery(pquery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
